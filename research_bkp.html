---
layout: layout
title: "Research Overview"
---

<h2 id='publications' class="page-heading">Publications</h1>


  <div class="divider"></div>


<div class="publication">
  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/mavis.png">
      <table>
        <tr>          
          <td><a href="https://openreview.net/forum?id=DFSb67ksVr">Paper</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Test-Time View Selection for Multi-Modal Decision Making </b> 
      <p> <b>Eeshaan Jain</b>, Johann Wenckstern, Benedikt von Querfurth, Charlotte Bunne <br /> Oral (Top 4%) @ MLGenX Workshop & Poster @ GemBio Workshop, ICLR 2025  </p>
    <p> The clinical routine has access to an ever-expanding repertoire of diagnostic tests, ranging from routine imaging to sophisticated molecular profiling technologies. Foundation models have recently emerged as powerful tools for extracting and integrating diagnostic information from these diverse clinical tests, advancing the idea of comprehensive patient digital twins. However, it remains unclear how to select and design tests that ensure foundation models can extract the necessary information for accurate diagnosis. We introduce MAVIS (Multi-modal Active VIew Selection), a reinforcement learning framework that unifies modality selection and feature selection into a single decision process. By leveraging foundation models, MAVIS dynamically determines which diagnostic tests to perform and in what sequence, adapting to individual patient characteristics. Experiments on real-world datasets across multiple clinical tasks demonstrate that MAVIS outperforms conventional approaches in both diagnostic accuracy and uncertainty reduction, while reducing testing costs by over 80%, suggesting a promising direction for optimizing clinical workflows through intelligent test design and selection. </p>
    
      <a href="#" class="bib-toggle" style="
    display: inline-block;
    margin-top: 1em;
    color: #000;
    text-decoration: none;
    font-size: 1em;
    cursor: pointer;
  ">
    <span class="toggle-icon">▶</span> <span class="toggle-text">Show BibTeX</span>
    </a>
    
    <!-- Hidden BibTeX code block styled as a neat square -->
    <div class="bib-block" style="
          display: none;
          margin-top: 1em;
          width: 100%;
          height: 100%;
          overflow: auto;
          border: 1px solid #ccc;
          padding: 10px;
          background-color: #f9f9f9;
          box-sizing: border-box;
        ">
      <pre style="margin: 0; font-family: monospace;">
@article{wenckstern2025ai,
title={AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery},
author={Wenckstern, Johann and Jain, Eeshaan and Vasilev, Kiril and Pariset, Matteo and Wicki, Andreas and Gut, Gabriele and Bunne, Charlotte},
journal={arXiv preprint arXiv:2501.06039},
year={2025}
}
        </pre>
      </div>
    </div>
  </div>
  <div class="divider"></div>
  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/virtues.pdf">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/pdf/2501.06039">Paper</a></td>
          <td><a href="https://github.com/bunnelab/virtues">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery </b> 
      <p> Johann Wenckstern*, <b>Eeshaan Jain*</b>, Kiril Vasilev, Matteo Pariset, Andreas Wicki, Gabriele Gut, Charlotte Bunne <br />ArXiv Preprint  </p>
    <p> 
      Spatial proteomics technologies have transformed our understanding of complex tissue architectures by enabling simultaneous analysis of multiple molecular markers and their spatial organization. The high dimensionality of these data, varying marker combinations across experiments and heterogeneous study designs pose unique challenges for computational analysis. Here, we present Virtual Tissues (VirTues), a foundation model framework for biological tissues that operates across the molecular, cellular and tissue scale. VirTues introduces innovations in transformer architecture design, including a novel tokenization scheme that captures both spatial and marker dimensions, and attention mechanisms that scale to high-dimensional multiplex data while maintaining interpretability. Trained on diverse cancer and non-cancer tissue datasets, VirTues demonstrates strong generalization capabilities without task-specific fine-tuning, enabling cross-study analysis and novel marker integration. As a generalist model, VirTues outperforms existing approaches across clinical diagnostics, biological discovery and patient case retrieval tasks, while providing insights into tissue function and disease mechanisms.
    </p>
    <a href="#" class="bib-toggle" style="
    display: inline-block;
    margin-top: 1em;
    color: #000;
    text-decoration: none;
    font-size: 1em;
    cursor: pointer;
  ">
    <span class="toggle-icon">▶</span> <span class="toggle-text">Show BibTeX</span>
    </a>
    
    <!-- Hidden BibTeX code block styled as a neat square -->
    <div class="bib-block" style="
          display: none;
          margin-top: 1em;
          width: 100%;
          height: 100%;
          overflow: auto;
          border: 1px solid #ccc;
          padding: 10px;
          background-color: #f9f9f9;
          box-sizing: border-box;
        ">
      <pre style="margin: 0; font-family: monospace;">
@article{wenckstern2025ai,
title={AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery},
author={Wenckstern, Johann and Jain, Eeshaan and Vasilev, Kiril and Pariset, Matteo and Wicki, Andreas and Gut, Gabriele and Bunne, Charlotte},
journal={arXiv preprint arXiv:2501.06039},
year={2025}
}
        </pre>
      </div>
    </div>
  </div>
</div>

<div class="divider"></div>


  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/clique-demo.png">
      <table>
        <tr>          
          <td><a href="https://openreview.net/forum?id=DFSb67ksVr">Paper</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Clique Number Estimation via Differentiable Functions of Adjacency Matrix Permutations </b> 
      <p> Indradyumna Roy*, <b>Eeshaan  Jain*</b>, Soumen Chakrabarti, Abir De <br /> ICLR 2025  </p>
    <p> MxNet is a fully differentiable clique number estimator that learns from distant supervision without explicit clique demonstrations. We reformulate MCP as detecting dense submatrices via learned permutations within a nested subgraph matching task.
       </p>
       
    </div>
  </div>

  <div class="divider"></div>


  <div class="publication">
  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/ged-leakage.pdf">
      <table>
        <tr>          
          <td><a href="https://openreview.net/pdf?id=guapIeLs02">Paper</a></td>
	        <td><a href="https://anonymous.4open.science/r/GED-Datasets/">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Graph Edit Distance Evaluation Datasets: Pitfalls and Mitigation </b> 
      <p> <b>Eeshaan  Jain*</b>, Indradyumna Roy*, Saswat Meher, Soumen Chakrabarti, Abir De <br /> LoG 2024 (Extended Abstract)  </p>
    <p> 
      Graph Edit Distance (GED) is a powerful framework for modeling both symmetric and asymmetric relationships between graph pairs under various cost settings. Due to the combinatorial intractability of exact GED computation, recent advancements have focused on neural GED estimators that approximate GED by leveraging data distribution characteristics. However, the datasets commonly used to benchmark such neural models exhibit two critical flaws: (1) significant isomorphism bias and (2) reliance on uniform edit costs for GED ground truths. Our datasets eliminate isomorphism leakage and incorporate a range of edit costs, facilitating more accurate assessment of GED methods
    </p>
    <!-- Button to toggle BibTeX -->
    <a href="#" class="bib-toggle" style="
    display: inline-block;
    margin-top: 1em;
    color: #000;
    text-decoration: none;
    font-size: 1em;
    cursor: pointer;
  ">
    <span class="toggle-icon">▶</span> <span class="toggle-text">Show BibTeX</span>
    </a>
    
    <!-- Hidden BibTeX code block styled as a neat square -->
    <div class="bib-block" style="
          display: none;
          margin-top: 1em;
          width: 100%;
          height: 100%;
          overflow: auto;
          border: 1px solid #ccc;
          padding: 10px;
          background-color: #f9f9f9;
          box-sizing: border-box;
        ">
      <pre style="margin: 0; font-family: monospace;">
@inproceedings{
jain2024graph,
title={Graph Edit Distance Evaluation Datasets: Pitfalls and Mitigation},
author={Eeshaan Jain and Indradyumna Roy and Saswat Meher and Soumen Chakrabarti and Abir De},
booktitle={The Third Learning on Graphs Conference},
year={2024},
url={https://openreview.net/forum?id=guapIeLs02}
}
        </pre>
      </div>
    </div>
  </div>
</div>
 
  


<div class="divider"></div>

<div class="publication">
  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/GRAPHEDX.png">
      <table>
        <tr>          
          <td><a href="https://openreview.net/forum?id=u7JRmrGutT">Paper</a></td>
	        <td><a href="https://github.com/structlearning/GraphEdX">Code</a></td>
          <td><a href="https://arxiv.org/abs/2409.17687">Arxiv</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Graph Edit Distance  with General Costs  Using Neural Set Divergence </b> 
      <p> <b>Eeshaan  Jain*</b>, Indradyumna Roy*, Saswat Meher, Soumen Chakrabarti, Abir De <br /> NeurIPS 2024 and LoG 2024 (Extended Abstract)  </p>
    <p> GraphEdx is the first-of-its-kind neural GED framework that incorporates variable edit costs, capable of modeling both symmetric and asymmetric graph (dis)similarities, allowing for more flexible and accurate GED estimation compared to earlier methods. </p>
    <!-- Button to toggle BibTeX -->
    <a href="#" class="bib-toggle" style="
    display: inline-block;
    margin-top: 1em;
    color: #000;
    text-decoration: none;
    font-size: 1em;
    cursor: pointer;
  ">
    <span class="toggle-icon">▶</span> <span class="toggle-text">Show BibTeX</span>
    </a>
    
    <!-- Hidden BibTeX code block styled as a neat square -->
    <div class="bib-block" style="
          display: none;
          margin-top: 1em;
          width: 100%;
          height: 100%;
          overflow: auto;
          border: 1px solid #ccc;
          padding: 10px;
          background-color: #f9f9f9;
          box-sizing: border-box;
        ">
      <pre style="margin: 0; font-family: monospace;">
@inproceedings{
anonymous2024graph,
title={Graph Edit Distance with General Costs Using Neural Set Divergence},
author={Eeshaan Jain and Indradyumna Roy and Saswat Meher and Soumen Chakrabarti and Abir De},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=u7JRmrGutT},
  }
        </pre>
      </div>
    </div>
  </div>
</div>


<div class="divider"></div>

<div class="publication">
  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/subselnet.pdf">
      <table>
        <tr>          
          <td><a href="https://openreview.net/forum?id=q3fCWoC9l0">Paper</a></td>
	        <td><a href="https://github.com/structlearning/subselnet">Code</a></td>
          <td><a href="https://arxiv.org/pdf/2409.12255">Arxiv</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks </b> 
      <p> <b>Eeshaan  Jain</b>, Tushar Nandy, Gaurav Aggarwal, Ashish V. Tendulkar, Rishabh K Iyer, Abir De <br /> NeurIPS 2023 </p>
    <p> 
      Existing subset selection methods for efficient learning predominantly employ discrete combinatorial and model-specific approaches, which lack generalizability--- for each new model, the algorithm has to be executed from the beginning. We propose `SubSelNet`, a non-adaptive subset selection framework, which tackles these problems.</p>  
    <!-- Button to toggle BibTeX -->
    <a href="#" class="bib-toggle" style="
    display: inline-block;
    margin-top: 1em;
    color: #000;
    text-decoration: none;
    font-size: 1em;
    cursor: pointer;
  ">
    <span class="toggle-icon">▶</span> <span class="toggle-text">Show BibTeX</span>
    </a>
    
    <!-- Hidden BibTeX code block styled as a neat square -->
    <div class="bib-block" style="
          display: none;
          margin-top: 1em;
          width: 100%;
          height: 100%;
          overflow: auto;
          border: 1px solid #ccc;
          padding: 10px;
          background-color: #f9f9f9;
          box-sizing: border-box;
        ">
      <pre style="margin: 0; font-family: monospace;">
@article{jain2023efficient,
title={Efficient data subset selection to generalize training across models: Transductive and inductive networks},
author={Jain, Eeshaan and Nandy, Tushar and Aggarwal, Gaurav and Tendulkar, Ashish and Iyer, Rishabh and De, Abir},
journal={Advances in Neural Information Processing Systems},
volume={36},
pages={4716--4740},
year={2023}
}
        </pre>
      </div>
    </div>
  </div>
</div>

  <script>
    // Attach click event listeners to all elements with the "bib-toggle" class
    document.querySelectorAll('.bib-toggle').forEach(function(toggle) {
      toggle.addEventListener('click', function(e) {
        e.preventDefault();
        
        // Find the closest publication container, then the bib-block inside it
        var publication = toggle.closest('.publication');
        var bibBlock = publication.querySelector('.bib-block');
        var icon = toggle.querySelector('.toggle-icon');
        var text = toggle.querySelector('.toggle-text');
  
        // Toggle display and update icon/text
        if (bibBlock.style.display === "none" || bibBlock.style.display === "") {
          bibBlock.style.display = "block";
          icon.textContent = "▼";
          text.textContent = "Hide BibTeX";
        } else {
          bibBlock.style.display = "none";
          icon.textContent = "▶";
          text.textContent = "Show BibTeX";
        }
      });
    });
  </script>