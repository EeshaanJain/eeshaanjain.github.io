---
layout: layout
title: "Research Overview"
---

<h2 id='publications' class="page-heading">Publications</h1>

  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/virtues.pdf">
      <table>
        <tr>          
          <td><a href="https://arxiv.org/pdf/2501.06039">Paper</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery </b> 
      <p> Johann Wenckstern*, <b>Eeshaan Jain*</b>, Kiril Vasilev, Matteo Pariset, Andreas Wicki, Gabriele Gut, Charlotte Bunne <br />ArXiv Preprint  </p>
    <p> 
      Spatial proteomics technologies have transformed our understanding of complex tissue architectures by enabling simultaneous analysis of multiple molecular markers and their spatial organization. The high dimensionality of these data, varying marker combinations across experiments and heterogeneous study designs pose unique challenges for computational analysis. Here, we present Virtual Tissues (VirTues), a foundation model framework for biological tissues that operates across the molecular, cellular and tissue scale. VirTues introduces innovations in transformer architecture design, including a novel tokenization scheme that captures both spatial and marker dimensions, and attention mechanisms that scale to high-dimensional multiplex data while maintaining interpretability. Trained on diverse cancer and non-cancer tissue datasets, VirTues demonstrates strong generalization capabilities without task-specific fine-tuning, enabling cross-study analysis and novel marker integration. As a generalist model, VirTues outperforms existing approaches across clinical diagnostics, biological discovery and patient case retrieval tasks, while providing insights into tissue function and disease mechanisms.
    </p>
    </div>
  </div>

  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/clique-demo.png">
      <table>
        <tr>          
          <td><a href="https://openreview.net/forum?id=DFSb67ksVr">Paper</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Clique Number Estimation via Differentiable Functions of Adjacency Matrix Permutations </b> 
      <p> Indradyumna Roy*, <b>Eeshaan  Jain*</b>, Soumen Chakrabarti, Abir De <br /> ICLR 2025  </p>
    <p> MxNet is a fully differentiable clique number estimator that learns from distant supervision without explicit clique demonstrations. We reformulate MCP as detecting dense submatrices via learned permutations within a nested subgraph matching task. </p>
    </div>
  </div>

 
  
  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/GRAPHEDX.png">
      <table>
        <tr>          
          <td><a href="https://openreview.net/forum?id=u7JRmrGutT">Paper</a></td>
	        <td><a href="https://github.com/structlearning/GraphEdX">Code</a></td>
          <td><a href="https://arxiv.org/abs/2409.17687">Arxiv</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Graph Edit Distance  with General Costs  Using Neural Set Divergence </b> 
      <p> <b>Eeshaan  Jain*</b>, Indradyumna Roy*, Saswat Meher, Soumen Chakrabarti, Abir De <br /> NeurIPS 2024 and LoG 2024 (Extended Abstract)  </p>
    <p> GraphEdx is the first-of-its-kind neural GED framework that incorporates variable edit costs, capable of modeling both symmetric and asymmetric graph (dis)similarities, allowing for more flexible and accurate GED estimation compared to earlier methods. </p>
    <!-- Button to toggle BibTeX -->
    <a href="#" onclick="toggleBibtex(); return false;" style="
      display: inline-block;
      margin-top: 0em;
      color: #000;
      text-decoration: none;
      font-size: 1em;
      cursor: pointer;
    ">
      <span id="toggleIcon">▶</span> <span id="toggleText">Show BibTeX</span>
    </a>
    
    <!-- Hidden BibTeX code block styled as a neat square -->
    <div id="bibtexBlock" style="
      display: none;
      margin-top: 1em;
      width: 100%;
      height: 100%;
      overflow: auto;
      border: 1px solid #ccc;
      padding: 10px;
      background-color: #f9f9f9;
      box-sizing: border-box;
      ">
      <pre style="margin: 0; font-family: monospace;">
@inproceedings{
anonymous2024graph,
title={Graph Edit Distance with General Costs Using Neural Set Divergence},
author={Eeshaan Jain and Indradyumna Roy and Saswat Meher and Soumen Chakrabarti and Abir De},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=u7JRmrGutT},
  }
        </pre>
      </div>
    </div>
  </div>

 
  <div class="divider"></div>



  <script>
    function toggleBibtex() {
      var bibtexDiv = document.getElementById("bibtexBlock");
      var toggleIcon = document.getElementById("toggleIcon");
      var toggleText = document.getElementById("toggleText");
      if (bibtexDiv.style.display === "none") {
        bibtexDiv.style.display = "block";
        toggleIcon.textContent = "▼";
        toggleText.textContent = "Hide BibTeX";
      } else {
        bibtexDiv.style.display = "none";
        toggleIcon.textContent = "▶";
        toggleText.textContent = "Show BibTeX";
      }
    }
  </script>
  